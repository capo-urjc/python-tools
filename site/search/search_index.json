{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Bienvenido a la Documentaci\u00f3n de python-tools \u00b6 Este proyecto es una herramienta escrita en Python que hace ... Esta documentaci\u00f3n est\u00e1 dise\u00f1ada para ayudarte a entender c\u00f3mo usar, configurar y desarrollar la herramienta. Contenido \u00b6 Introducci\u00f3n Instalaci\u00f3n Referencias de la API Introducci\u00f3n \u00b6 breve descripci\u00f3n del proyecto, sus objetivos y c\u00f3mo puede ayudar a los usuarios. Instalaci\u00f3n \u00b6 Para instalar el proyecto, puedes usar: ```bash pip install ...","title":"Home"},{"location":"#bienvenido-a-la-documentacion-de-python-tools","text":"Este proyecto es una herramienta escrita en Python que hace ... Esta documentaci\u00f3n est\u00e1 dise\u00f1ada para ayudarte a entender c\u00f3mo usar, configurar y desarrollar la herramienta.","title":"Bienvenido a la Documentaci\u00f3n de python-tools"},{"location":"#contenido","text":"Introducci\u00f3n Instalaci\u00f3n Referencias de la API","title":"Contenido"},{"location":"#introduccion","text":"breve descripci\u00f3n del proyecto, sus objetivos y c\u00f3mo puede ayudar a los usuarios.","title":"Introducci\u00f3n"},{"location":"#instalacion","text":"Para instalar el proyecto, puedes usar: ```bash pip install ...","title":"Instalaci\u00f3n"},{"location":"api/","text":"API Reference \u00b6 CAVEDataset \u00b6 Bases: Dataset Dataset class for loading Hyperspectral images dataset (CAVE). Source code in src/capo_tools/datasets/CAVE/dataset.py class CAVEDataset(Dataset): \"\"\" Dataset class for loading Hyperspectral images dataset (CAVE). \"\"\" def __init__(self, root_dir: str, patch_size: tuple, transform=None, download=False, mode='train'): \"\"\" Initializes the dataset by loading the data. Args: root_dir (str): Root directory where the dataset is stored. patch_size (tuple): Size of the patch to be extracted from the images. transform (callable, optional): Optional transform to be applied on the samples. download (bool, optional): If True, downloads the dataset from the internet if it's not already downloaded. mode (str, optional): Mode of the dataset ('train', 'valid', 'test'.). \"\"\" if not os.path.exists(root_dir): print('Downloading CAVE dataset:') download_CAVE(path_to_save=root_dir) if download: if not os.path.exists(root_dir): print('Downloading CAVE dataset:') download_CAVE(path_to_save=root_dir) else: print('Downloading and restoring CAVE dataset:') shutil.rmtree(root_dir) download_CAVE(path_to_save=root_dir) self.root_dir = os.path.join(root_dir, mode) self.patch_size = patch_size self.transform = transform self.images: list = [] images_paths: list = sorted(os.listdir(self.root_dir)) for path in images_paths: numpy_image: np.ndarray = np.load(os.path.join(self.root_dir, path)) padded_image: np.ndarray = np.pad(numpy_image, (self.patch_size, self.patch_size, (0, 0)), 'constant', constant_values=0) self.images.append(padded_image) print('CAVE dataset loaded.') def __len__(self): \"\"\" Get the total number of images in the dataset. Returns: int: Total number of images. \"\"\" return len(self.images) def __getitem__(self, idx): \"\"\" Get a sample from the dataset at the specified index. Args: idx (int): Index of the sample to retrieve. Returns: torch.Tensor: Transformed sample. \"\"\" if torch.is_tensor(idx): idx = idx.tolist() idx = idx % len(self.images) image = self.images[idx] image: np.ndarray = extract_random_patch(image, self.patch_size) image = image.astype(np.float32) if self.transform: sample = self.transform(image) else: sample = transforms.ToTensor()(image) return sample __getitem__(idx) \u00b6 Get a sample from the dataset at the specified index. Parameters: Name Type Description Default idx int Index of the sample to retrieve. required Returns: Type Description torch.Tensor: Transformed sample. Source code in src/capo_tools/datasets/CAVE/dataset.py def __getitem__(self, idx): \"\"\" Get a sample from the dataset at the specified index. Args: idx (int): Index of the sample to retrieve. Returns: torch.Tensor: Transformed sample. \"\"\" if torch.is_tensor(idx): idx = idx.tolist() idx = idx % len(self.images) image = self.images[idx] image: np.ndarray = extract_random_patch(image, self.patch_size) image = image.astype(np.float32) if self.transform: sample = self.transform(image) else: sample = transforms.ToTensor()(image) return sample __init__(root_dir, patch_size, transform=None, download=False, mode='train') \u00b6 Initializes the dataset by loading the data. Parameters: Name Type Description Default root_dir str Root directory where the dataset is stored. required patch_size tuple Size of the patch to be extracted from the images. required transform callable Optional transform to be applied on the samples. None download bool If True, downloads the dataset from the internet if it's not already downloaded. False mode str Mode of the dataset ('train', 'valid', 'test'.). 'train' Source code in src/capo_tools/datasets/CAVE/dataset.py def __init__(self, root_dir: str, patch_size: tuple, transform=None, download=False, mode='train'): \"\"\" Initializes the dataset by loading the data. Args: root_dir (str): Root directory where the dataset is stored. patch_size (tuple): Size of the patch to be extracted from the images. transform (callable, optional): Optional transform to be applied on the samples. download (bool, optional): If True, downloads the dataset from the internet if it's not already downloaded. mode (str, optional): Mode of the dataset ('train', 'valid', 'test'.). \"\"\" if not os.path.exists(root_dir): print('Downloading CAVE dataset:') download_CAVE(path_to_save=root_dir) if download: if not os.path.exists(root_dir): print('Downloading CAVE dataset:') download_CAVE(path_to_save=root_dir) else: print('Downloading and restoring CAVE dataset:') shutil.rmtree(root_dir) download_CAVE(path_to_save=root_dir) self.root_dir = os.path.join(root_dir, mode) self.patch_size = patch_size self.transform = transform self.images: list = [] images_paths: list = sorted(os.listdir(self.root_dir)) for path in images_paths: numpy_image: np.ndarray = np.load(os.path.join(self.root_dir, path)) padded_image: np.ndarray = np.pad(numpy_image, (self.patch_size, self.patch_size, (0, 0)), 'constant', constant_values=0) self.images.append(padded_image) print('CAVE dataset loaded.') __len__() \u00b6 Get the total number of images in the dataset. Returns: Name Type Description int Total number of images. Source code in src/capo_tools/datasets/CAVE/dataset.py def __len__(self): \"\"\" Get the total number of images in the dataset. Returns: int: Total number of images. \"\"\" return len(self.images)","title":"API"},{"location":"api/#api-reference","text":"","title":"API Reference"},{"location":"api/#capo_tools.datasets.CAVE.dataset.CAVEDataset","text":"Bases: Dataset Dataset class for loading Hyperspectral images dataset (CAVE). Source code in src/capo_tools/datasets/CAVE/dataset.py class CAVEDataset(Dataset): \"\"\" Dataset class for loading Hyperspectral images dataset (CAVE). \"\"\" def __init__(self, root_dir: str, patch_size: tuple, transform=None, download=False, mode='train'): \"\"\" Initializes the dataset by loading the data. Args: root_dir (str): Root directory where the dataset is stored. patch_size (tuple): Size of the patch to be extracted from the images. transform (callable, optional): Optional transform to be applied on the samples. download (bool, optional): If True, downloads the dataset from the internet if it's not already downloaded. mode (str, optional): Mode of the dataset ('train', 'valid', 'test'.). \"\"\" if not os.path.exists(root_dir): print('Downloading CAVE dataset:') download_CAVE(path_to_save=root_dir) if download: if not os.path.exists(root_dir): print('Downloading CAVE dataset:') download_CAVE(path_to_save=root_dir) else: print('Downloading and restoring CAVE dataset:') shutil.rmtree(root_dir) download_CAVE(path_to_save=root_dir) self.root_dir = os.path.join(root_dir, mode) self.patch_size = patch_size self.transform = transform self.images: list = [] images_paths: list = sorted(os.listdir(self.root_dir)) for path in images_paths: numpy_image: np.ndarray = np.load(os.path.join(self.root_dir, path)) padded_image: np.ndarray = np.pad(numpy_image, (self.patch_size, self.patch_size, (0, 0)), 'constant', constant_values=0) self.images.append(padded_image) print('CAVE dataset loaded.') def __len__(self): \"\"\" Get the total number of images in the dataset. Returns: int: Total number of images. \"\"\" return len(self.images) def __getitem__(self, idx): \"\"\" Get a sample from the dataset at the specified index. Args: idx (int): Index of the sample to retrieve. Returns: torch.Tensor: Transformed sample. \"\"\" if torch.is_tensor(idx): idx = idx.tolist() idx = idx % len(self.images) image = self.images[idx] image: np.ndarray = extract_random_patch(image, self.patch_size) image = image.astype(np.float32) if self.transform: sample = self.transform(image) else: sample = transforms.ToTensor()(image) return sample","title":"CAVEDataset"},{"location":"api/#capo_tools.datasets.CAVE.dataset.CAVEDataset.__getitem__","text":"Get a sample from the dataset at the specified index. Parameters: Name Type Description Default idx int Index of the sample to retrieve. required Returns: Type Description torch.Tensor: Transformed sample. Source code in src/capo_tools/datasets/CAVE/dataset.py def __getitem__(self, idx): \"\"\" Get a sample from the dataset at the specified index. Args: idx (int): Index of the sample to retrieve. Returns: torch.Tensor: Transformed sample. \"\"\" if torch.is_tensor(idx): idx = idx.tolist() idx = idx % len(self.images) image = self.images[idx] image: np.ndarray = extract_random_patch(image, self.patch_size) image = image.astype(np.float32) if self.transform: sample = self.transform(image) else: sample = transforms.ToTensor()(image) return sample","title":"__getitem__"},{"location":"api/#capo_tools.datasets.CAVE.dataset.CAVEDataset.__init__","text":"Initializes the dataset by loading the data. Parameters: Name Type Description Default root_dir str Root directory where the dataset is stored. required patch_size tuple Size of the patch to be extracted from the images. required transform callable Optional transform to be applied on the samples. None download bool If True, downloads the dataset from the internet if it's not already downloaded. False mode str Mode of the dataset ('train', 'valid', 'test'.). 'train' Source code in src/capo_tools/datasets/CAVE/dataset.py def __init__(self, root_dir: str, patch_size: tuple, transform=None, download=False, mode='train'): \"\"\" Initializes the dataset by loading the data. Args: root_dir (str): Root directory where the dataset is stored. patch_size (tuple): Size of the patch to be extracted from the images. transform (callable, optional): Optional transform to be applied on the samples. download (bool, optional): If True, downloads the dataset from the internet if it's not already downloaded. mode (str, optional): Mode of the dataset ('train', 'valid', 'test'.). \"\"\" if not os.path.exists(root_dir): print('Downloading CAVE dataset:') download_CAVE(path_to_save=root_dir) if download: if not os.path.exists(root_dir): print('Downloading CAVE dataset:') download_CAVE(path_to_save=root_dir) else: print('Downloading and restoring CAVE dataset:') shutil.rmtree(root_dir) download_CAVE(path_to_save=root_dir) self.root_dir = os.path.join(root_dir, mode) self.patch_size = patch_size self.transform = transform self.images: list = [] images_paths: list = sorted(os.listdir(self.root_dir)) for path in images_paths: numpy_image: np.ndarray = np.load(os.path.join(self.root_dir, path)) padded_image: np.ndarray = np.pad(numpy_image, (self.patch_size, self.patch_size, (0, 0)), 'constant', constant_values=0) self.images.append(padded_image) print('CAVE dataset loaded.')","title":"__init__"},{"location":"api/#capo_tools.datasets.CAVE.dataset.CAVEDataset.__len__","text":"Get the total number of images in the dataset. Returns: Name Type Description int Total number of images. Source code in src/capo_tools/datasets/CAVE/dataset.py def __len__(self): \"\"\" Get the total number of images in the dataset. Returns: int: Total number of images. \"\"\" return len(self.images)","title":"__len__"}]}